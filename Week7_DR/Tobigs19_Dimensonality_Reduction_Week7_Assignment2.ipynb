{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차원축소 Assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data에 적용을 해보기\n",
    "\n",
    "mnist-original.mat 파일과 같은 위치에서 주피터 노트북을 열어주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy import io\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/2a/c1aa78b0211b1e3f6439d4edabd44bfb6e1c45544a7a241e3ba5cfe3c316/tensorflow_datasets-4.8.2-py3-none-any.whl (5.3MB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (4.1.1)\n",
      "Collecting etils[enp,epath]>=0.9.0 (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ac/4f4b4096acd0160e0895715b47974b1f304b5e4a6b5169ce8d1355820eb4/etils-0.9.0-py3-none-any.whl (140kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Collecting dill (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (3.19.6)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/cb/1a1dd9e8495d3932c056120127d18695bca50c2f754013b6bf0d51557c47/tensorflow_metadata-1.12.0-py3-none-any.whl (52kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/68/5474b81cbef0e6a0fc9f10e0e73bd02840c759a997de8ec61b1e04292d6f/dm_tree-0.1.8-cp37-cp37m-win_amd64.whl (102kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (1.11.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (8.1.3)\n",
      "Collecting importlib-resources; python_version < \"3.9\" (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/38/71/c13ea695a4393639830bf96baea956538ba7a9d06fcce7cef10bfff20f72/importlib_resources-5.12.0-py3-none-any.whl\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (1.21.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (5.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (4.31.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\datascience\\lib\\site-packages (from tensorflow_datasets) (2.2.0)\n",
      "Requirement already satisfied: zipp; extra == \"epath\" in c:\\users\\user\\datascience\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.15.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/4e/ed585842aaa704d87495a0e99317aaa44c5007a597c05b995fa8cfc4dfbe/googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\user\\datascience\\lib\\site-packages (from click->tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\user\\datascience\\lib\\site-packages (from click->tensorflow_datasets) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\datascience\\lib\\site-packages (from promise->tensorflow_datasets) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\user\\datascience\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\datascience\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\datascience\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2019.3.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\user\\datascience\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\19\\49\\34\\c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
      "Successfully built promise\n",
      "Installing collected packages: importlib-resources, etils, dill, googleapis-common-protos, tensorflow-metadata, dm-tree, promise, toml, tensorflow-datasets\n",
      "Successfully installed dill-0.3.6 dm-tree-0.1.8 etils-0.9.0 googleapis-common-protos-1.58.0 importlib-resources-5.12.0 promise-2.3 tensorflow-datasets-4.8.2 tensorflow-metadata-1.12.0 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정보!\n",
    "- 7만개의 작은 숫자 이미지\n",
    "- 행 열이 반대로 되어있음 -> 전치\n",
    "- grayscale 28x28 pixel = 784 feature\n",
    "- 각 picel은 0~255의 값\n",
    "- label = 1~10 label이 총 10개인거에 주목하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data를 각 픽셀에 이름붙여 표현\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df에 라벨 y를 붙여서 데이터프레임 생성\n",
    "df['y'] = y\n",
    "print('Size of the dataframe: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 형태 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "# Plot the graph\n",
    "plt.gray()\n",
    "fig = plt.figure( figsize=(16,7) )\n",
    "for i in range(0,15):\n",
    "    ax = fig.add_subplot(3,5,i+1, title=\"Digit: {}\".format(str(df.loc[rndperm[i],'y'])) )\n",
    "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) train_test_split을 통해 데이터를 0.8 0.2의 비율로 분할 해 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5b5da9685bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m808\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scaled_X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = 0.2, random_state=808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기서부터는 원본데이터 & PCA 축소 데이터 & LDA 축소 데이터 비교해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 차원축소 기법(PCA와 LDA)을 이용하여 mnist data를 축소시켜주세요\n",
    "\n",
    "pca를 이용할 때는, 주성분 개수를 정하는 과정에 대해 잘 서술해주시면 좋겠죠!<br>\n",
    "강의에서 배웠던 3가지 중 어떤 걸 고려해서 갯수를 정했는지요!!!!<br>\n",
    "scree plot같은거는 직접 그려서 확인해주면 좋겠죠???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 지금까지 배웠던 머신러닝 기법을 이용하여 학습해주세요 (2개이상 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  이때 time stamp를 찍어서 training 시간을 비교하고, test accuracy도 비교해주세요\n",
    "#### (원본 데이터 & PCA 축소 데이터 & LDA 축소 데이터 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정한 틀 없이 자유롭게 해주시면 됩니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
